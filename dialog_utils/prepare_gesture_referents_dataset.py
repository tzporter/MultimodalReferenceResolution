import seaborn as sns
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import colorcet as cc
import torch
import whisper
from tqdm import tqdm
# import spacy
from tqdm import tqdm
from dialog_utils.read_labeled_and_pos_target_data import prepare_dialogue_shared_expressions_and_turns_info
from dialog_utils.data_classes import Turn, Gesture, Utterance
# from dialog_utils.prepare_annotate_dialogues import get_gestures_info, get_turn_info


# def get_gestures_and_speech_info(turns_info):
#    pairs_name = ['pair10', 'pair17', 'pair15', 'pair21', 'pair18', 'pair24', 'pair16', 'pair13', 'pair07', 'pair22', 'pair20',  'pair08', 'pair05', 'pair11', 'pair23', 'pair04', 'pair09', 'pair12', 'pair14' ]
#    gestures_and_speech_info = []
#    for pair in pairs_name:
#       pair_turns_info = turns_info[pair]
#       for turn in pair_turns_info:
#          if turn.gesture:
#             for gesture in turn.gesture:
#                gesture_info = {'pair': pair, 'turn': turn.ID, 'round': turn.round, 'type': gesture.g_type, 'is_gesture': gesture.is_gesture, 'from_ts': gesture.g_from_ts, 'to_ts': gesture.g_to_ts, 'hand': gesture.g_hand, 'referent': gesture.g_referent, 'comment': gesture.g_comment, 'object': turn.target, 'utterance': turn.utterance, 'speaker': turn.speaker, 'trial': turn.trial, 'director': turn.director, 'correct_answer': turn.correct_answer, 'given_answer': turn.given_answer, 'accuracy': turn.accuracy, 'processed_utterance': turn.processed_utterance, 'pos_sequence': turn.pos_sequence, 'lemmas_sequence': turn.lemmas_sequence, 'text_lemma_pos': turn.text_lemma_pos, 'turn_from_ts': turn.from_ts, 'turn_to_ts': turn.to_ts}
#                gestures_and_speech_info.append(gesture_info)
#    gestures_and_speech_info = pd.DataFrame(gestures_and_speech_info)
#    gestures_and_speech_info['duration'] = gestures_and_speech_info['to_ts'] - gestures_and_speech_info['from_ts']
#    gestures_and_speech_info = gestures_and_speech_info.fillna('')
#    gesture_data = gestures_and_speech_info.groupby('type').count().reset_index().rename(columns={'pair': 'count'})[['type', 'count']]
#    gesture_data['normalized_count'] = gesture_data['count'] / gesture_data['count'].sum()
#    gesture_data['normalized_count'] = gesture_data['normalized_count'].apply(lambda x: round(x, 2))
#    # fill nan values in gestures_info with empty strings
#    gestures_and_speech_info = gestures_and_speech_info.fillna('')
#    # group gestures by pair, turn, type, is_gesture, from_ts, to_ts, referent, comment, and duration and other attributes
#    gestures_and_speech_info = gestures_and_speech_info.groupby(['pair', 'turn', 'round', 'type', 'is_gesture', 'from_ts', 'to_ts', 'referent', 'comment', 'duration', 'object', 'utterance', 'speaker', 'trial', 'director', 'correct_answer', 'given_answer', 'accuracy', 'processed_utterance', 'pos_sequence', 'lemmas_sequence', 'text_lemma_pos', 'turn_from_ts', 'turn_to_ts']).agg({'hand': lambda x: ', '.join(x)}).reset_index()
#    gestures_and_speech_info = gestures_and_speech_info[gestures_and_speech_info['is_gesture'] == 'gesture']
   
#    aligned_tagged_speech_per_word_small = pd.read_csv('dialog_utils/dialign/dialogues/lg/aligned_tagged_speech_per_word_small.csv')
#    buffer = 0.5
#    contain_text = np.bool_(np.zeros(len(gestures_and_speech_info)))
#    words = np.zeros(len(gestures_and_speech_info), dtype=object)
#    lemmas = np.zeros(len(gestures_and_speech_info), dtype=object)
#    pos = np.zeros(len(gestures_and_speech_info), dtype=object)
#    lemmas_pos = np.zeros(len(gestures_and_speech_info), dtype=object)
#    transcriptions = np.zeros(len(gestures_and_speech_info), dtype=object)
#    # reset gestures_and_speech_info
#    gestures_and_speech_info.reset_index(drop=True, inplace=True)

#    gestures_and_speech_info['pair_speaker'] = gestures_and_speech_info['pair'] + '_' + gestures_and_speech_info['speaker']
#    unique_speakers = gestures_and_speech_info['pair_speaker'].unique()
    
#    use_model = True
#    audio_dict = {}
#    # get unique speakers
#    for a_speaker in tqdm(unique_speakers):
#       pair, speaker = a_speaker.split('_')
#       pair_speaker = f"{pair}_{speaker}"
#       audio_path = os.path.join("data", f"{pair}_synced_pp{speaker}.wav")
#       input_audio = whisper.load_audio(audio_path)
#       audio_dict[pair_speaker] = input_audio
#    model = whisper.load_model("large-v3")
#    nlp = spacy.load("nl_core_news_lg")
#    model.forced_decoder_ids = None
#    model.eval()
#    # model to gpu
#    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#    model.to(device)
#    options = whisper.DecodingOptions()
        
#    for row_index, row in tqdm(gestures_and_speech_info.iterrows(), total=len(gestures_and_speech_info)):
#       pair = row['pair']
#       speaker = row['speaker']
#       start_time = row['from_ts']
#       end_time = row['to_ts']
#       speech_end_frame = end_time + buffer
#       speech_start_frame = start_time - buffer
#       # check which rows in aligned_tagged_speech_per_word_small are within the speech_start_frame and speech_end_frame
#       speech_info = aligned_tagged_speech_per_word_small[(aligned_tagged_speech_per_word_small['pair_name'] == pair) & (aligned_tagged_speech_per_word_small['speaker'] == speaker) & (aligned_tagged_speech_per_word_small['from_ts'] >= speech_start_frame) & (aligned_tagged_speech_per_word_small['to_ts'] <= speech_end_frame)]
#       if not speech_info.empty:
#          words[row_index] = ' '.join(speech_info['word'])
#          lemmas[row_index] = ' '.join(speech_info['lemma'])
#          pos[row_index] = ' '.join(speech_info['pos'])
#          lemmas_pos[row_index] = ' '.join(speech_info['lemma_pos'])
#          transcriptions[row_index] = 'manual'
#          contain_text[row_index] = True
#       else:
#          print("Automatic transcription")
#          try:
#                sr = 16000
#                audio_segment = audio_dict[f'{pair}_{speaker}'][int(speech_start_frame*sr):int(speech_end_frame*sr)]
#                input_features = whisper.pad_or_trim(audio_segment)
#                mel = whisper.log_mel_spectrogram(input_features, n_mels=128).to(device)
#                # decode the audio
#                result = whisper.decode(model, mel, options)
#                text = result.text
#                contain_text[row_index] = bool(text)
#                if bool(text):
#                   print(f"{pair_speaker}: {text} for {speech_start_frame} to {speech_end_frame}")
                  
#                pos_utterance = nlp(str(text))
#                lemmas_with_pos = ''
#                pos_sequence = ''
#                lemmas_sequence = ''
#                text_lemma_pos = ''
#                utterance_idx = 0
#                final_utterance = []
#                for idx, token in enumerate(pos_utterance):
#                   lemmas_sequence += token.lemma_+' '
#                   pos_sequence += token.pos_+' '
#                   lemmas_with_pos += token.lemma_+'#'+token.pos_+' '
#                   text_lemma_pos += token.text+'_'+token.lemma_+'#'+token.pos_+' '
               
#                words[row_index] = text        
#                lemmas[row_index] = lemmas_sequence 
#                pos[row_index] = pos_sequence
#                lemmas_pos[row_index] = lemmas_with_pos
#                transcriptions[row_index] = 'automatic'   
         
#          except Exception as e:
#                print(f"Error processing sample {row_index} with error {e}")
#                words[row_index] = ''
#                contain_text[row_index] = False
#    gestures_and_speech_info['words'] = words
#    gestures_and_speech_info['lemmas'] = lemmas
#    gestures_and_speech_info['pos'] = pos
#    gestures_and_speech_info['lemmas_pos'] = lemmas_pos
#    gestures_and_speech_info['transcriptions'] = transcriptions
#    gestures_and_speech_info['contain_text'] = contain_text 
#    return gestures_and_speech_info, gesture_data

def get_detailed_gestures(gestures_path='./dialog_utils/data/gestures_referents_info.csv'):
   # first check if the file exists
   try:
      gestures_and_speech_info = pd.read_csv(gestures_path)
      return gestures_and_speech_info
   except:
      print('File not found. Generating the detailed gestures info...')
   # code to set up the paths
   dialign_output = './dialog_utils/dialign/dialogues/lg_output/'
   turn_info_path = './dialog_utils/dialign/dialogues/lg/'
   fribbles_path = "./dialog_utils/data/Fribbles/{}.jpg"
   videos_path = '~/data/{}_synced_overview.mp4'

   pos_func_words = ['DET', 'PRON', 'ADP', 'CCONJ', 'SCONJ', 'AUX', 'PART', 'PUNCT', 'SYM', 'X', 'INTJ', 'NUM', 'SPACE']
   content_word_pos = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'ADV']

   shared_constructions_info, turns_info = prepare_dialogue_shared_expressions_and_turns_info(dialign_output, turn_info_path)
   speech_and_objects = []
   for pair in turns_info.keys():
      for this_turn in turns_info[pair]:
         speech_and_objects.append({'utterance': this_turn.utterance, 'object': this_turn.target, 'pair': pair, 'turn_ID': this_turn.ID, 'speaker': this_turn.speaker, 'utterance': this_turn.utterance, 'target': this_turn.target, 'trial': this_turn.trial, 'director': this_turn.director, 'correct_answer': this_turn.correct_answer, 'given_answer': this_turn.given_answer, 'accuracy': this_turn.accuracy, 'pos_sequence': this_turn.pos_sequence, 'lemmas_sequence': this_turn.lemmas_sequence, 'text_lemma_pos': this_turn.text_lemma_pos, 'round': this_turn.round, 'from_ts': this_turn.from_ts, 'to_ts': this_turn.to_ts})
   speech_and_objects = pd.DataFrame(speech_and_objects)
   
   gestures_and_speech_info, gesture_data = get_gestures_and_speech_info(turns_info)
   # check duplicates in gestures_and_speech_info (using the 'pair', 'turn', 'round', 'type', 'is_gesture', 'from_ts', 'to_ts')
   duplicates = gestures_and_speech_info[gestures_and_speech_info.duplicated(subset=['pair', 'turn', 'round', 'type', 'is_gesture', 'from_ts', 'to_ts'], keep=False)]
   # remove duplicates from gestures_and_speech_info
   gestures_and_speech_info = gestures_and_speech_info.drop_duplicates(subset=['pair', 'turn', 'round', 'type', 'is_gesture', 'from_ts', 'to_ts'], keep='first')
   duplicates = gestures_and_speech_info[gestures_and_speech_info.duplicated(subset=['pair', 'from_ts', 'to_ts'], keep=False)]
   gestures_and_speech_info = gestures_and_speech_info.drop_duplicates(subset=['pair', 'from_ts', 'to_ts'], keep='first')
   # reset index
   gestures_and_speech_info = gestures_and_speech_info.reset_index(drop=True)
   gestures_and_speech_info['segmentation method'] = 'manual'
   # rename utterance column to speech
   gestures_and_speech_info = gestures_and_speech_info.rename(columns={'utterance': 'speech'})
    
   # save gestures_and_speech_info to a csv file
   gestures_and_speech_info.to_csv(gestures_path, index=False)
   return gestures_and_speech_info

if __name__ == '__main__':
   gestures_and_speech_info = get_detailed_gestures()
   print(gestures_and_speech_info.head())

   
Experiment_name: unimodal_{}_lr_{}_batch_{}_temperature_{}_final
# feeder, currently no preprocessing implemented so that we run the first evaluation using sequences of segments
feeder_args:
  data_path: './speaker_segments.csv'
  poses_path: data/selected_poses/poses-{}.npy
  audio_path: data/audio_files/{}_synced_pp{}.wav
  apply_skeleton_augmentations: True
  n_views: 2
  debug: False
  random_choose: True
  random_shift: True
  window_size: 72
  normalization: True
  random_mirror: True
  random_mirror_p: 0.5
  is_vector: False
  skeleton_backbone: 'jointsformer'
  eval_on_small_dataset: False
  use_only_small_dataset: False
  
# model for embeddings
model_args:
  modalities: ['skeleton']
  fusion: 'late'
  feat_dim: 128
  w2v2_type: 'multilingual'
  skeleton_backbone: 'jointsformer'
  hidden_dim: 256
  attentive_pooling: False
  attentive_pooling_skeleton: False
  bertje_dim: 768
  freeze_bertje: True
  loss_types: ['contrastive', 'masked_reconstruction'] # ['contrastive', 'mm_contrastive', 'masked_reconstruction', 'vicreg', 'mm_vicreg']
  cross_modal: False
  one_branch_cross_modal: False
  multimodal_embeddings_dim: 768

# augmenation file
skeleton_augmentations_path: 'configs/augmentations/skeleton_simple_aug.yaml'

# model for audio
audio_model: model.audio_model.Wav2Vec2
audio_model_args:
  w2v2_type: 'multilingual'
  freeze: True

# training
device: -1
keep_rate: 0.9
batch_size: 64
num_epoch: 10
nesterov: True
num_workers: 8
momentum: 0.9
learning_rate: 0.0003
lr_rate_decay: 0.1
lr_decay_epochs: 700,800,900
weight_decay: 0.0001
# vicreg hyperparameters from the paper
sim_coeff: 25
std_coeff: 25
cov_coeff: 1


# general config
accumulate_grad_batches: 1
scheduler: 'plateau'

loss_function: 'Combined'

# for the contrastive loss
temp: 0.1

# masked reconstruction hyperparameters
mask_ratio: 0.05
mask_T_ratio: 0.05
